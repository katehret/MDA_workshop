---
output:
  pdf_document: default
  html_document: default
---
# R commands for factor analysis and related statistics utilised in Ehret and Taboada (2021). "Characterising online news comments: A multi-dimensional cruise through online registers".

## Adapted version used in the Workshop "How to do Multi-dimensional Analysis: Theoretical background and practical analyses in R" May 22 and 23, 2024, held at SFU Burnaby


## load libraries
```{r}
library(tidyverse)
library(psych)
#library(ggplot2)
library(reshape2)

```


## prepare frequency matrix

unzip the archive normalized_postag_counts.zip

read in matrix containing all normalised frequencies per feature as columns (read in as data.frame)
```{r}
data <- read.csv("data/normalized_postag_counts/normalized_postag_counts_renamed.csv")
```

let's take a look at the data
```{r}
head(data)
```

set file names as rownames
```{r}
rownames(data) <- data$file_names
```

remove first column containing non-numerical data

```{r}
fa.data <- data[,-(1)] 
```

order features alphabetically

```{r}
fa.data <- fa.data[, order(names(fa.data))]
```

## test data for suitability/factorability

Kaiser-Meyer-Oelkin measure for sample adequacy (MSA): calculates the amount of intercorrelations among the variables; ranges from 0 to 1 with 1 indicating that each variable can be perfectly predicted by the other variables; the overall MSA should be at least greater than .5; individual variables that are below that threshhold might be removed

```{r}
KMO(fa.data)
```

Overall MSA = .77 this is "middling" and we can proceed

Bartlett's test of sphericity: based on the entire correlation matrix; tests for significant correlations (sensitive to sample size)

this requires the number of files in the dataset

```{r}
length(data$file_names)
```

calculate correlation matrix
```{r}
fa.cor <- cor(fa.data)

cortest.bartlett(fa.cor, n=41586) #n = number of files in dataset

```

p-value = 0 hence we can proceed

## determine number of factors

eigenvalues and scree plot

calculate eigenvalues based on the correlation matrix
```{r}
fa.eigen <- eigen(fa.cor) #eigenvalues are the sum of squared factor loadings: sum(factorLoadings^2)

ev <- fa.eigen$values

```

create scree plot of eigenvalues equal or greater than 1.1

```{r}
factor <- c(1:12)
eigenvalue <- ev[1:12]

df <- data.frame(factor, eigenvalue)

p <- ggplot(df, aes(reorder(factor, -eigenvalue), eigenvalue, group=1))
p <- p + geom_bar(stat="identity", fill= rgb(0,0,0.4))
p <- p + geom_line(color = rgb(1,0,0), linetype = "solid")+ geom_point(shape=19, color=rgb(1,0,0)) + xlab("Factor") + ylab("Eigenvalue") +
theme_bw()

p <- p + geom_text(label = round(eigenvalue, 2), vjust=-0.4, hjust = 0)

p

#ggsave("screeplot_eigenvalues.pdf", dpi=300, units="mm", width=180)

#ggsave("screeplot_eigenvalues.jpg", dpi=300, units="mm", width=180)
```


calculate percentage of variance explained and store in data.frame
```{r}
cumsum(ev) #calculate cumulative sum of eigenvalues

#calculate proportion of variance explained by each factor
var <- ev/sum(ev)*100

#calculate cumulative proportion of variance explained in percent
cumVar <- cumsum(ev)/sum(ev)*100

dimension <- c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12")

variance <- var[1:12]

df <- data.frame(dimension, variance)

```


make plot with percentage of variance for each factor

```{r}
p2 =  ggplot(df, aes(reorder(dimension, -variance), variance, group=1))

p2 <- p2 + geom_bar(stat="identity", fill= rgb(0,0,0.4))

p2 <- p2 + geom_line(color = rgb(1,0,0), linetype = "solid")+ geom_point(shape=19,
color=rgb(1,0,0)) + xlab("Factors") + ylab("Percentage of variance explained") +
theme_bw()

p2 <- p2 + geom_text(label = round(variance, 1), vjust=-0.4, hjust = 0)

p2

```


## factor analysis 

step-wise adding factors to decide how many factors to extract; inspect number of salient loadings (loadings greater than .3); we need at least 5 for a factor to be interpretable

```{r}
fa1 <- factanal(x = fa.data, factors = 1, rotation = "promax", method="mle")

fa2 <- factanal(x = fa.data, factors = 2, rotation = "promax", method="mle")

fa3 <- factanal(x = fa.data, factors = 3, rotation = "promax", method="mle")

fa4 <- factanal(x = fa.data, factors = 4, rotation = "promax", method="mle")

fa5 <- factanal(x = fa.data, factors = 5, rotation = "promax", method="mle")

```


create data frame with loadings for likely candidates

```{r}
#fa2
loads2 <- round(fa2$loadings, 2)
 
loadings2 <- as.data.frame(unclass(loads2))

#save to file for inspection
write.csv(loadings2, "data/loadings2.csv")


#fa3
loads3 <- round(fa3$loadings, 3)
 
loadings3 <- as.data.frame(unclass(loads3))

#save to file for inspection
write.csv(loadings3, "data/loadings3.csv")

```


calculate and extract factor scores for final factor solution

```{r}
fa3 = factanal(x = fa.data, factors = 3, rotation = "promax", method="mle",
scores="regression")

factorScores = as.data.frame(fa3$scores)

#add register information
registers <- read.csv("texts_by_register.csv")
factorScores$register <- registers$register

#save to file
write.csv(factorScores, "data/factorScores.csv")

```


calculate mean factor scores for each register 

```{r}
#take column means
byRegister <- factorScores %>% group_by(register) %>% summarise_if(is.numeric, mean, na.rm =T)

#save to file
write.csv(byRegister, "data/meanFactorScores.csv", row.names=F)

```

calculate standard deviation for mean factor scores
```{r}
sd_meanfactorScores <- factorScores %>% group_by(register) %>%
summarise_if(is.numeric, list(mean = ~mean(., na.rm=T), sd = ~sd(., na.rm=T)))

#save to file
#write.csv(sd_meanfactorScores, "data/sd_meanFactorScores.csv", row.names=F)

```

